# Installation
## Notes before you follow
이 작업을 수행하면 아래와 같은 패키지가 설치됩니다. 기존 시스템에 구성된 설정에 따라 의존성 오류가 발생할 수 있으며 이는 심각한 시스템 충돌을 초래합니다. 절대 운용중인 실제 서버에서의 테스트 하지 마십시오. 새로운 Instance에서 테스트 후 운영하시는 시스템 설정에 맞게 설정을 변경하고 일정한 테스트 기간을 가진 뒤에 실제 서버에 적용하십시오.

* Essentials
* Cuda Toolkit 7.0
* cuDNN Toolkit 6.5
* Java 8
* Bazel 0.1.4
* TensorFlow 0.6

설치과정에서 예기지 못한 오류가 발생할 수 있습니다. 설치 및 설정중에 화면에 표시되는 로그를 항상 확인하시고 메모해두십시오.

## Configure AWS for installation
> 단순한 설치순서나 명령어의 차이로 오류가 발생하거나 다른 결과값이 출력될 수 있습니다. 아래 내용은 TensorFlow 포럼에서 테스트를 거친 올바른 방법입니다. 아래 방법대로 설치를 진행해주시기 바랍니다.

1. AWS Console에 로그인 합니다.
> AWS를 사용하시려면 계정 설정에서 사용가능한 신용카드(체크카드도 가능)를 등록하십시오. 잔액이 부족하거나 신용카드가 등록되지 않으면 Instance가 강제종료되거나 삭제될 수 있습니다.

2. 오른쪽 상단에서 Region을 US East(N. Virginia)로 변경합니다.
> Console 화면에서 GPU Instance를 지원하는 Region으로 변경해야 GPU EC2 Instance를 사용할 수 있습니다.
>>South Korea Region 지원여부는 2017년 상반기에 발표될 예정이라고 합니다.

3. 왼쪽 상단에서 Amazon EC2를 클릭하여 EC2 서비스로 이동합니다.

4. Create Instance 항목에서 파란색 Launch Instance 버튼을 클릭합니다.
> Service Health 에서 모든 서비스가 사용가능한 상태인지 꼭 확인하십시오. Scheduled Events에 일정이 등록된 경우 해당 기간동안 서비스가 일시적으로 중단되거나 다른 Region으로 Live Migration 될 수도 있습니다.

5. Quick Start 탭에서 "Ubuntu Server 14.04 LTS (HVM), SSD Volume Type" 을 찾아 Select 버튼을 클릭하여 다음 페이지로 이동합니다.
> Ubuntu Server 14.04 LTS AMI 으로 선택하십시오. AMI(Amazon Machine Image)세부설명에 Cuda 가 설치된 AMI도 있습니다. 절대 선택하지 마십시오. Clean 한 ubuntu AMI만을 선택하여야 합니다.

6. Instance Type 목록에서 g2.2xlarge 를 찾아 선택하고 하단의 Next: Configure Instance Details 를 클릭하여 다음 단계로 이동합니다.
> 이 단계부터는 절대 건너뛰지 마십시오. 처음부터 다시 시작하셔야 할 수도 있습니다.

7. 아무런 조작도 하지 않고 하단의 Next: Add Storage를 눌러 다음 단계로 이동합니다.
> 필요에 따라 화면을 스크린샷 하셔서 보관하시는 것도 좋습니다.

8. Add Storage 단계에서는 필요에 따라 Storage용량을 설정해주시기 바랍니다. 이 시스템은 최소 50GB의 용량을 필요로 합니다. 설정이 끝나면 하단의 Next: Add Tags를 눌러 다음 단계로 이동합니다.
> 너무 적은 Storage용량을 설정해도 문제이지만, 너무 많은 Storage용량을 설정해도 추후 많은 비용이 과금될 수 있습니다. 나중에 Storage를 Add 할 수 있습니다. 당장 사용해야할 용량만 계산해주시기 바랍니다.

9. 따로 설정이 필요하지 않습니다. 필요에 따라 Tag를 수정하시고 하단의 Next: Configure Security Group를 눌러 다음 단계로 이동합니다.

10. Configure Security Group 단계에서는 방화벽 설정을 적용하는 단계입니다. 기본적으로 SSH연결에 필요한 TCP 22번 포트가 개방되어 있도록 설정되어 있는게 정상입니다. 설정되어 있지 않다면 설정해주시기 바랍니다. FTP를 사용하시려면 이 단계에서 20번과 21번 TCP 연결을 개방으로 설정하시기 바랍니다.
> SSH연결을 위한 22번 포트 설정을 삭제하실 경우 원격으로 접속하실 수 없습니다. 이 설정은 나중에 변경하실 수 있습니다.

11. 오른쪽 하단의 버튼을 눌러 Instance 설정을 최종적으로 확인하시고, 구성이 올바르게 되었다면 오른쪽 하단을 버튼을 한 번 더 눌러 Instance를 생성하시기 바랍니다.
> EC2 Instance 생성을 하셨다면 잠시 기다리시기 바랍니다. 상태등이 초록색으로 바뀌게 되면 ubuntu설치가 정상적으로 완료되어 접속이 가능한 상태인 것입니다.

12. SSH 접속을 위한 Key파일 생성이 필요합니다.

## TensorFlow Installation

1. 인스턴스가 생성되면 필수요소를 설치합니다. 설치는 약 3분정도 소요됩니다.
```
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install -y build-essential git python-pip libfreetype6-dev libxft-dev libncurses-dev libopenblas-dev gfortran python-matplotlib libblas-dev liblapack-dev libatlas-base-dev python-dev python-pydot linux-headers-generic linux-image-extra-virtual unzip python-numpy swig python-pandas python-sklearn unzip wget pkg-config zip g++ zlib1g-dev
sudo pip install -U pip
```

2. Cuda Toolkit을 설치합니다.
```
wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1410/x86_64/cuda-repo-ubuntu1410_7.0-28_amd64.deb
sudo dpkg -i cuda-repo-ubuntu1410_7.0-28_amd64.deb
rm cuda-repo-ubuntu1410_7.0-28_amd64.deb
sudo apt-get update
sudo apt-get install -y cuda
```

3. 여기까지 설치를 진행했다면 시스템을 재부팅 합니다.
```
sudo reboot
```

4. Nvidia 공식 사이트에서 cuDNN 를 다운받아 설치합니다. 이 단계는 Nvidia 공식 계정이 필요합니다.
[여기를 눌러 다운로드하시고](https://developer.nvidia.com/rdp/assets/cudnn-65-linux-v2-asset)
파일을 다운로드 받은 뒤에 파일명은 "cudnn-6.5-linux-x64-v2.tgz"으로 수정합니다.
```
tar -zxf cudnn-6.5-linux-x64-v2.tgz && rm cudnn-6.5-linux-x64-v2.tgz
sudo cp -R cudnn-6.5-linux-x64-v2/lib* /usr/local/cuda/lib64/
sudo cp cudnn-6.5-linux-x64-v2/cudnn.h /usr/local/cuda/include/
```

5. 여기까지 설치를 진행했다면 시스템을 재부팅 합니다.
> 에러가 발생한 경우 메모해둔 다음 재부팅 후 다시 시도해보시기 바랍니다. 그래도 문제가 발생한다면 구글이나 Stackoverflow 에서 문제를 검색해보시기 바랍니다.
```
sudo reboot
```

6. 재부팅이 완료되면 "~/.bashrc" 으로 몇가지 환경변수를 수정합니다.
```
export CUDA_HOME=/usr/local/cuda
export CUDA_ROOT=/usr/local/cuda
export PATH=$PATH:$CUDA_ROOT/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_ROOT/lib64
```

7. Java 8 버전을 설치합니다.
```
sudo add-apt-repository -y ppa:webupd8team/java
sudo apt-get update
# Hack to silently agree license agreement
echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
sudo apt-get install -y oracle-java8-installer
```

8. bazel 을 설치합니다.
```
sudo apt-get install pkg-config zip g++ zlib1g-dev
https://github.com/bazelbuild/bazel/releases/download/0.1.4/bazel-0.1.4-installer-linux-x86_64.sh
chmod +x bazel-0.1.4-installer-linux-x86_64.sh
./bazel-0.1.4-installer-linux-x86_64.sh --user
rm bazel-0.1.4-installer-linux-x86_64.sh
```

9. github 에서 tensorflow 프로젝트를 clone 시킵니다.
```
git clone --recurse-submodules https://github.com/tensorflow/tensorflow
cd tensorflow
```

10. 비공식 설정을 통해 CUDA 버전 3.0을 사용하여 GPU 지원으로 TensorFlow를 빌드 할 예정입니다.
```
TF_UNOFFICIAL_SETTING=1 ./configure
```

11. 아래와 같은 안내문이 나오면 3.0 을 입력하여 설치된 CUDA 버전이 3.0 임을 설정합니다.  **꼭 올바른 버전을 설치/입력하십시오.**
> Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: "3.5,5.2"]: 3.0<br><br> **3.0 을 입력하지 않고 무시할 경우 아래와 같은 에러가 표시됩니다.**<br><br>
> Ignoring gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.

12. 아래 명령을 실행합니다. 이 작업은 대략 25분정도 소요됩니다.
```
bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer
bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow-0.6.0-cp27-none-linux_x86_64.whl
```

13. 거의다 완료하였습니다! Python에서 다음 명령어를 수행하면 GPU가 올바르게 설정되었다고 표시될 것 입니다!
```
import tensorflow as tf
tf_session = tf.Session()
x = tf.constant(1)
y = tf.constant(1)
tf_session.run(x + y)
```

14. TensorFlow가 MNIST 데이터 세트에서 CNN을 학습시켜서 올바르게 작동하는지 확인할 수도 있습니다.
```
python ~/tensorflow/tensorflow/models/image/mnist/convolutional.py
# Lots of output followed by GPU-related things...
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 3.66GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x7023e0000 extends to 0x7ec556000
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB
...
Initialized!
Epoch 0.00
Minibatch loss: 12.053, learning rate: 0.010000
Minibatch error: 90.6%
Validation error: 84.6%
Epoch 0.12
Minibatch loss: 3.282, learning rate: 0.010000
Minibatch error: 6.2%
Validation error: 6.9%
Epoch 0.23
Minibatch loss: 3.466, learning rate: 0.010000
Minibatch error: 12.5%
Validation error: 3.7%
Epoch 0.35
Minibatch loss: 3.191, learning rate: 0.010000
Minibatch error: 7.8%
Validation error: 3.4%
Epoch 0.47
Minibatch loss: 3.201, learning rate: 0.010000
Minibatch error: 4.7%
Validation error: 2.7%
...
```

15. 설치를 정상적으로 마쳤습니다! 더이상의 연산이 필요없다면, 과금을 방지하기 위해 AWS 에서 해당 Instance의 전원을 Stop 하십시오.

** 이 문서에 문제가 있다고 생각되시는 경우 Github 이슈를 열어서 문제상황을 구체적으로 설명해주시면 확인 후 문서 내용을 수정하겠습니다. 감사합니다. **
